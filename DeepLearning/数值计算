1. 上溢与下溢
        连续数学在数字计算机上的根本困难是：我们需要通过有限数量的位模式来表示无限实数。这意味着几乎总会引入近似误差，很多情况这仅仅是舍入误差。
    设计算法中一定要考虑舍入误差的累计问题。
        一种极具毁灭性的舍入误差是下溢。当接近0的数被四舍五入之后，会得到0的结果，而很多函数在参数为0而不是一个很小的正数时，会表现出质的不同。
    例如除法，分母要避免为0。
        另一种错误形式是上溢。当大量级的数被近似为∞或-∞时发生上溢。

2. 病态条件
        条件数：函数相对于输入的微小变化而变化的快慢程度。输入被轻微扰动而迅速改变的函数对于科学计算来说可能是有问题的，因为四舍五入可能导致输出
    的巨大变化。如f(x) = A-1x, A具有特征分解，条件数为max(特征值i / 特征值j)。当该数很大时，矩阵求逆对输入的误差特别敏感。从计算公式可以知道
    矩阵的条件数是矩阵本身的固有特征，所以即使是正确的矩阵，也可能因为四舍五入放大预先存在的误差。
    
3. 基于梯度的优化方法
        优化指的是改变x以最小化或最大化某个函数f(x)的任务。通常以最小化指代大多数最优化问题。
        最小化或最大化的函数称为目标函数或准则。当进行最小化时也称为代价函数/损失函数/误差函数，通常使用上标*表示最小化或最大化函数的x值。如记
   x* = arg min f(x)。
        记f'(x)为f(x)的导数，f(x-a sign(f'(x)))当a足够小时，其值是比f(x)小的。通过这种方式移动x的值，进而渐渐减少f(x)的值，这种技术称为
   梯度下降。显然当f'(x)=0时，导数无法提供移动方向，这个点称为临界点或驻点。使f(x)取得绝对最小值的点是全局最小点。
        针对多维输入的函数，移动方向由梯度决定，梯度是f的所有偏导组成的向量，记为▼xf(x)。沿着负梯度向量移动可以减少f，这被称为最速下降法或梯度
    下降。即x' = x - a ▼xf(x)。 a即为学习率，是一个确定步长的正标量。有很多方法选择a,可以直接选择小常数，也可以计算多个a对应的结果，选择结果
    值最小的a(线搜索）。

4. 梯度之上：Jacobian和Hessian矩阵
        当计算输入和输出都是向量的函数的所有偏导数，包含所有这样的偏导数的矩阵被称为Jacobian矩阵。当我们有多维输入时，二阶导数也有许多，将这些
   导数合并成一个矩阵，称为Hessian矩阵（二阶导，先对x求导，在对y求导，和先对y求导，再对x求导，结果相同，所以是对称阵）。
        由泰勒二阶展开，可以知道，f(x) = f(x0) + (x-x0)T g + 1/2 (x-x0)T H(x-x0)。其中T代表转置运算，g代表梯度，H代表X0点的Hessian。
   则就可以得到新的梯度下降方法 f(x(0) - a g) = f(x(0)) - a gTg + 1/2 a*a gTHg。所以当最后一项过大时，梯度下降实际上是可能向上移动的，
   当gTHg为负时，则增加a将永远使f下降。gTHg为正，a* = gTg / gTHg。当要最小化函数能用二次函数近似的情况下，Hessian的特征值决定了学习率的量
   级。
        使用Hessian矩阵的信息来指导搜索，以解决一个问题，其中最简单的方法是牛顿法。即基于一个二阶泰勒展开来近似。
                f(x) = f(x(0)) + (x-x(0))T▼xf(x(0)) + 1/2 (x-x(0))T H(f)(x(0))(x-x(0))
        通过计算，可以得到临界值：
                x* = x(0) - H(f)(x(0))^-1 ▼xf(x(0))
        仅使用梯度信息的优化算法称为一阶优化算法，如梯度下降。使用Hessian矩阵的优化算法称为二阶最优化算法，如牛顿法。
