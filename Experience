损失函数
=======

1. 交叉熵：H（P,Q) = - 求和（P(x)log(Q(x))
   理解为描述同一变量x概率分布Q相对于P估计的准确程度，所以在使用交叉熵损失函数时，一般会设定P代表的是正确答案，而Q代表的是预测的结果值。
   tensorflow中没有计算交叉熵的函数，但是可以通过已有函数组合
   i. clip_by_value(t, clip_value_min, clip_value_max) 截取向量t，t中小于min的用min代替，大于max的用max代替，其他不变
   ii. reduce_mean(input_tensor, axisk keep_dims, name, reduction_indices) 对向量中的元素求和，然后求平均值。/
   交叉熵：
      cross_entropy = -tf.reduce_mean( y_ * tf.log( tf.clip_by_value( y, 1e-10, 1.0 ) ) )
      (PS: y_为真值， y为预测值 tensorflow中 * 表示矩阵对应位置元素相乘，matmul（）为矩阵乘法）
      
2. 交叉熵主要用于分类问题，而回归问题，预测值没有概率分布，所以不能用交叉熵，主要是均方误差损失函数。

3. 变量的关系分为两类：一是确定性问题，二是非确定性问题，即所谓的相关关系。

4. 自定义损失函数，假设一个工厂需要预测某种商品的出货量以制订生产计划，但是如果预测量多一个，就会损失一件商品的成本（b元）
   如果，预测少一件，就会少转一件的利润（a元），所以显然有两个维度的损失函数，所以可以采用分段的损失函数：
                       a(y - y') y > y'
         f(yi, yi') =
                       b(y' - y) y <= y'
   loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * a, (y' - y) * b))  

5. J(w)代表损失函数，J‘(w)代表损失函数在w处的导数，那么当参数r足够小时，就能保证J(w - r*sign(J'(w)))一定比J(w)小。
   sign(x<0)=-1, sign(x>0)=1, sign(x=0)=0。用sign可以保证w-r*sign(J'(w))一定是往损失函数减少的放心移动；r则
   是学习率，用于控制每次反馈之后的移动程度，即学习样本的程度。

6. 当数据样本很大时，直接用梯度下降，数据计算成本很高，收敛速度慢，一般采用随机梯度下降的方法，具体是：每次只用部分样本
   （batch）进行训练，这样收敛速度快，最终结果也能达到预期的结果。
   
7. 自适应学习率算法：思路：如果损失与某一指定参数的偏导的符号相同，那么学习率应该增加；反之，如果不同，则应该减小。

8. train.GradientDescentOptimizer是梯度下降优化器，需要全部样本进行训练，速度比较慢；train.AdagradOptimizer是
   Adagrad自适应学习率优化器；train.RMSPropOptimizer自适应学习率优化器，实现RMSProp优化算法；....

9. 正则化是为了防止过拟合，没有正则化，直接优化损失函数J(w)可以将之变得特别小，这样容易过拟合；
   正则化，则是优化J(w) + a*R(w)（a一般很小，如0.01）,后面部分就是正则化项，这样能保证不会去完全拟合数据。
   可以理解为，用正则项去拟合噪声，这样就不会将噪声拟合到模型中了
   
10. L1正则化会让参数变得更稀疏（会有更多的参数变为0），而L2正则化不会。由于L2可导，但是L1不可导，所以L2计算更加简洁。
   实践中，L1和L2有时候同时使用。
   tensorflow L1正则化项：contrib.layers.l1_regularizer(), L2正则化项：contrib.layers.l2_regularizer()
