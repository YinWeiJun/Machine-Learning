理解为什么需要研究GCN：
====================
        离散卷积本质就是一种加权求和。CNN中的卷积本质上就是利用一个共享参数的过滤器（kernel），通过计算中心像素点以及相邻像素点的加权和
        --------------------------
    来构成feature map实现空间特征的提取，当然加权系数就是卷积核的权重系数。那么卷积核的系数如何确定的呢？是随机化初值，然后根据误差函数
                                                                      --------------------------------------
    通过反向传播梯度下降进行迭代优化。这是一个关键点，卷积核的参数通过优化求出才能实现特征提取的作用，GCN的理论很大一部分工作就是为了引入
    ------------------------------
    可以优化的卷积参数。
    
        CNN处理的图像或者视频数据中像素点（pixel）是排列成成很整齐的矩阵，也就是很多论文中所提到的Euclidean Structure。与之相对应，科
        
    学研究中还有很多Non Euclidean Structure的数据，如图3所示。社交网络、信息网络中有很多类似的结构。实际上，这样的网络结构（Non Euclidean
    
    Structure）就是图论中抽象意义上的拓扑图。所以，Graph Convolutional Network中的Graph是指数学（图论）中的用顶点和边建立相应关系的拓
    
    扑图。那么为什么要研究GCN？
    
        (1)CNN无法处理Non Euclidean Structure的数据，学术上的表达是传统的离散卷积在Non Euclidean Structure的数据上无法保持平移不变性。
        ------------------------------------------
    通俗理解就是在拓扑图中每个顶点的相邻顶点数目都可能不同，那么当然无法用一个同样尺寸的卷积核来进行卷积运算。
    
        (2)由于CNN无法处理Non Euclidean Structure的数据，又希望在这样的数据结构（拓扑图）上有效地提取空间特征来进行机器学习，所以GCN成
        
    为了研究的重点。
    
        (3)读到这里大家可能会想，自己的研究问题中没有拓扑结构的网络，那是不是根本就不会用到GCN呢？其实不然，广义上来讲任何数据在赋范空间内
        
    都可以建立拓扑关联，谱聚类就是应用了这样的思想（谱聚类（spectral clustering）原理总结：https://www.cnblogs.com/pinard/p/6221564.html）。
                      -----
    所以说拓扑连接是一种广义的数据结构，GCN有很大的应用空间。

        综上所述，GCN是要为除CV、NLP之外的任务提供一种处理、研究的模型。
        
        
提取拓扑图空间特征的两种方式：
===========================
        GCN的本质目的就是用来提取拓扑图的空间特征，那么实现这个目标只有graph convolution这一种途径吗？当然不是，在vertex domain(spatial
        
    domain)和spectral domain实现目标是两种最主流的方式。
    
        (1)vertex domain(spatial domain)是非常直观的一种方式。顾名思义：提取拓扑图上的空间特征，那么就把每个顶点相邻的neighbors找出来。
        
    这里面蕴含的科学问题有二：
         -------------
            a.按照什么条件去找中心vertex的neighbors，也就是如何确定receptive field？

            b.确定receptive field，按照什么方式处理包含不同数目neighbors的特征？
        
        处理a,b两个问题可以参见：http://proceedings.mlr.press/v48/niepert16.pdf
                              ------------------------------------------------
        这种方法主要的缺点如下：
                     ---
            c.每个顶点提取出来的neighbors不同，使得计算处理必须针对每个顶点

            d.提取特征的效果可能没有卷积好
            
        (2)spectral domain就是GCN的理论基础了。这种思路就是希望借助图谱的理论来实现拓扑图上的卷积操作。从整个研究的时间进程来看：首先研究
        
    GSP（graph signal processing）的学者定义了graph上的Fourier Transformation，进而定义了graph上的convolution，最后与深度学习结合提出
    
    了Graph Convolutional Network。

        什么是Spectral graph theory？简单的概括就是借助于图的拉普拉斯矩阵的特征值和特征向量来研究图的性质。
        -------------------------------------------------------------------------------------------
        GCN为什么要利用Spectral graph theory？这应该是看论文过程中读不懂的核心问题了，要理解这个问题需要大量的数学定义及推导，没有一定的
        -------------------------------------
    数学功底难以驾驭
    
Spectral graph实现了什么？
========================
        Graph Fourier Transformation及Graph Convolution的定义都用到图的拉普拉斯矩阵，那么首先来介绍一下拉普拉斯矩阵。
        
        对于图 G = (V, E), 其Laplacian 矩阵的定义为 L = D - A, 其中 L 为Laplacian矩阵，D是顶点的度矩阵（对角矩阵），对角线上元素依次为
        
    各个顶点的度，A是图的邻接矩阵。
    
        常用的拉普拉斯矩阵实际有三种：
            
            1. L = D - A 定义的Laplacian 矩阵更专业的名称叫Combinatorial Laplacian。
            
            2. L(sys) = D^(-1/2) L D^(-1/2) 定义的叫Symmetric normalized Laplacian，很多GCN的论文中应用的是这种拉普拉斯矩阵。
    
            3. L(rw) = D^(-1) A 定义的叫Random walk normalized Laplacian,有读者的留言说看到了Graph Convolution与Diffusion相似之处，
            
            当然从Random walk normalized Laplacian就能看出了两者确有相似之处（其实两者只差一个相似矩阵的变换.

        为什么GCN要用拉普拉斯矩阵？
            
        拉普拉斯矩阵矩阵有很多良好的性质，这里写三点我感触到的和GCN有关之处

            (1)拉普拉斯矩阵是对称矩阵，可以进行特征分解（谱分解），这就和GCN的spectral domain对应上了。

            (2)拉普拉斯矩阵只在中心顶点和一阶相连的顶点上（1-hop neighbor）有非0元素，其余之处均为0。

            (3)通过拉普拉斯算子与拉普拉斯矩阵进行类比。
            
        
Deep Learning中的Graph Convolution
==================================
    


