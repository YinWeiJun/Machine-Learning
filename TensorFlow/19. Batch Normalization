一、批量标准化(BN，Batch Normalization)
======================================
    1、BN 简介
    ----------
        a、协变量偏移问题
        
            我们知道，在统计机器学习中算法中，一个常见的问题是协变量偏移(Covariate Shift)，协变量可以看作是输入变量。一般的深度神经
                                                                                                             -------------
            网络都要求输入变量在训练数据和测试数据上的分布是相似的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。
            --------------------------------------------------------------------------------------------------------------
            传统的深度神经网络在训练时，随着参数的不算更新，中间每一层输入的数据分布往往会和参数更新之前有较大的差异，导致网络要去不断
                                     --------------------------------------------------------------------------------------
            的适应新的数据分布，进而使得训练变得异常困难，我们只能使用一个很小的学习速率和精调的初始化参数来解决这个问题。而且这个中间
            ---------------------------------------------------------------------------------------------------
            层的深度越大时，这种现象就越明显。由于是对层间数据的分析，也即是内部（internal），因此这种现象叫做内部协变量偏移(internal
            
            Covariate Shift)。
            
        b、解决方法
        
            为了解决这个问题，Sergey Ioffe’s 和 Christian Szegedy’s 在2015年首次提出了批量标准化（Batch Normalization，BN）的想法。
            
            该想法是：不仅仅对输入层做标准化处理，还要对每一中间层的输入(激活函数前)做标准化处理，使得输出服从均值为0，方差为1的正态分布，
                     ---------------------------------------------------------------------------------------------------------
            从而避免内部协变量偏移的问题。之所以称之为批标准化：是因为在训练期间，我们仅通过计算当前层一小批数据的均值和方差来标准化每一
            ---------------------------
            层的输入。BN 的具体流程如下图所示：首先，它将隐藏层的输出结果（如，第一层：Wh1.xWh1.x，状态值）在 batch 上进行标准化；然后，
            
            经过缩放(scale)和平移(shift)处理，最后经过 RELU 激活函数得到h1h1送入下一层（就像我们在数据预处理中将 x 进行标准化后送入神经
            
            网络的第一层一样）。
            
    2、BN 的优点
    ------------
        现在几乎所有的卷积神经网络都会使用批量标准化操作，它可以为我们的网络训练带来一系列的好处。具体如下：

        首先，通过对输入和中间网络层的输出进行标准化处理后，减少了内部神经元分布的改变，使降低了不同样本间值域的差异性，得大部分的数据都其
        
        处在非饱和区域，从而保证了梯度能够很好的回传，避免了梯度消失和梯度爆炸；
                           ----------------------------------------------
        其次，通过减少梯度对参数或其初始值尺度的依赖性，使得我们可以使用较大的学习速率对网络进行训练，从而加速网络的收敛；
                 ---------------------------------                                               --------------
        最后，由于在训练的过程中批量标准化所用到的均值和方差是在一小批样本(mini-batch)上计算的，而不是在整个数据集上，所以均值和方差会有一
        
        些小噪声产生，同时缩放过程由于用到了含噪声的标准化后的值，所以也会有一点噪声产生，这迫使后面的神经元单元不过分依赖前面的神经元单元。
                                                                                        ---------------------------------------
        所以，它也可以看作是一种正则化手段，提高了网络的泛化能力，使得我们可以减少或者取消 Dropout，优化网络结构。
        -------------------------------------------------------------------------------------------------
        
    3、BN 的使用注意事项
    -------------------
        BN 通常应用于输入层或任意中间层，且最好在激活函数前使用。
        
        使用 BN 的层不需要加 bias 项了，因为减去batch 个数据的均值 μBμB 后， 偏置项 bb 的作用会被抵消掉，其平移的功能将由ββ 来代替。
                   ------------------
        BN 的训练阶段均值和方差的求法：全连接层：对 batch 个数据内的每个对应节点(node)分别求均值和方差；二维卷积层：对 batch 个数据内的
             -----------------------
        每一个对应的特征图（feature map）求均值和方差。
        
        BN 的测试阶段均值和方差的求法：测试时，使用的是训练集的每一个 BN 层均值和方差，这样做是为了避免在测试阶段只有一个数据实例的情况，
             -----------------------
        因为这种情况下如果使用测试集本身的均值和方差的话，均值就是它本身，方差为 0，标准化后会得到 0，这样就没意义了。所以，我们在训练阶
        
        段使用滑动平均的方法来近似计算。
        
    4、TensorFLow 中的 BN 怎么使用
    -----------------------------
        ##### 设计计算图阶段 #####

        # 1、去掉 bias 项，且在此阶段不使用激活函数
        linear_output = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)
        conv_output = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False, activation=None)

        # 2、使用 tf.contrib.layers.batch_norm() 或者 tf.layers.batch_normalization()进行标准化处理
        # 注意要传入占位符 is_training 表明是训练阶段还是测试阶段
        bn_linear_output = tf.layers.batch_normalization(linear_output, training=is_training)
        bn_conv_output = tf.layers.batch_normalization(conv_output, training=is_training)

        # 3、将标准化后的值传递给激活函数 
        liner_layer = tf.nn.relu(bn_linear_output)
        conv_layer = tf.nn.relu(bn_conv_output)

        # 4、训练之前必须先更新所有 BN 层的 moving_mean 和 moving_variance，因为测试阶段要用到训练集的均值和方差
        # 另解：设置 tf.contrib.layers.batch_norm() 函数中的参数 updates_collections 值为 None 强制其原地更新，就不用加 with 语句中的控制依赖了
        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)


        *********************************************************************


        ##### 执行计算图阶段 #####

        # 1、添加一个布尔型占位符 is_training 来表明当前是训练阶段还是测试阶段
        is_training = tf.placeholder(tf.bool, name="is_training")

        # 2、训练阶段将feed_dict 中的参数 is_training 置为 True, 测试阶段置为 False
        sess.run(train_step, feed_dict={input_layer: batch_xs, 
                                        labels: batch_ys, 
                                        is_training: True})

        sess.run(accuracy, feed_dict={input_layer: mnist.validation.images,
                                      labels: mnist.validation.labels,
                                      is_training: False})

        
二、实例标准化(IN，Instance Normalization)
=========================================
    BN 和 IN 其实本质上是同一个东西，只是 IN 是作用于单张图片（对单个图片的所有像素求均值和标准差），但是 BN 作用于一个 batch
    
    （对一个batch里所有的图片的所有像素求均值和标准差）。但是为什么 IN 还会被单独提出，而且在 Style Transfer、GAN 等任务中大
    
    放异彩呢？ 
    
    通过调整 BN 统计量，或学习的参数 ββ 和 γγ，BN 可以用来做 domain adaptation Style Transfer 是一个把每张图片当成一个domain
    
    的 domain adaptation 问题IN 在训练和测试阶段都用，BN 只在训练阶段用，测试阶段用滑动平均值和方差。
    
----------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------

        import numpy as np
        import tensorflow as tf
        import matplotlib.pyplot as plt


        ACTIVATION = tf.nn.relu
        N_LAYERS = 7
        N_HIDDEN_UNITS = 30


        def fix_seed(seed=1):
            # reproducible
            np.random.seed(seed)
            tf.set_random_seed(seed)


        def plot_his(inputs, inputs_norm):
            # plot histogram for the inputs of every layer
            for j, all_inputs in enumerate([inputs, inputs_norm]):
                for i, input in enumerate(all_inputs):
                    plt.subplot(2, len(all_inputs), j*len(all_inputs)+(i+1))
                    plt.cla()
                    if i == 0:
                        the_range = (-7, 10)
                    else:
                        the_range = (-1, 1)
                    plt.hist(input.ravel(), bins=15, range=the_range, color='#FF5733')
                    plt.yticks(())
                    if j == 1:
                        plt.xticks(the_range)
                    else:
                        plt.xticks(())
                    ax = plt.gca()
                    ax.spines['right'].set_color('none')
                    ax.spines['top'].set_color('none')
                plt.title("%s normalizing" % ("Without" if j == 0 else "With"))
            plt.draw()
            plt.pause(0.01)


        def built_net(xs, ys, norm):
            def add_layer(inputs, in_size, out_size, activation_function=None, norm=False):
                # weights and biases (bad initialization for this case)
                Weights = tf.Variable(tf.random_normal([in_size, out_size], mean=0., stddev=1.))
                biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)

                # fully connected product
                Wx_plus_b = tf.matmul(inputs, Weights) + biases

                # normalize fully connected product
                if norm:
                    # Batch Normalize
                    fc_mean, fc_var = tf.nn.moments(
                        Wx_plus_b,
                        axes=[0],   # the dimension you wanna normalize, here [0] for batch
                                    # for image, you wanna do [0, 1, 2] for [batch, height, width] but not channel
                    )
                    scale = tf.Variable(tf.ones([out_size]))
                    shift = tf.Variable(tf.zeros([out_size]))
                    epsilon = 0.001

                    # apply moving average for mean and var when train on batch
                    ema = tf.train.ExponentialMovingAverage(decay=0.5)
                    def mean_var_with_update():
                        ema_apply_op = ema.apply([fc_mean, fc_var])
                        with tf.control_dependencies([ema_apply_op]):
                            return tf.identity(fc_mean), tf.identity(fc_var)
                    mean, var = mean_var_with_update()

                    Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, mean, var, shift, scale, epsilon)
                    # similar with this two steps:
                    # Wx_plus_b = (Wx_plus_b - fc_mean) / tf.sqrt(fc_var + 0.001)
                    # Wx_plus_b = Wx_plus_b * scale + shift

                # activation
                if activation_function is None:
                    outputs = Wx_plus_b
                else:
                    outputs = activation_function(Wx_plus_b)

                return outputs

            fix_seed(1)

            if norm:
                # BN for the first input
                fc_mean, fc_var = tf.nn.moments(
                    xs,
                    axes=[0],
                )
                scale = tf.Variable(tf.ones([1]))
                shift = tf.Variable(tf.zeros([1]))
                epsilon = 0.001
                # apply moving average for mean and var when train on batch
                ema = tf.train.ExponentialMovingAverage(decay=0.5)
                def mean_var_with_update():
                    ema_apply_op = ema.apply([fc_mean, fc_var])
                    with tf.control_dependencies([ema_apply_op]):
                        return tf.identity(fc_mean), tf.identity(fc_var)
                mean, var = mean_var_with_update()
                xs = tf.nn.batch_normalization(xs, mean, var, shift, scale, epsilon)

            # record inputs for every layer
            layers_inputs = [xs]

            # build hidden layers
            for l_n in range(N_LAYERS):
                layer_input = layers_inputs[l_n]
                in_size = layers_inputs[l_n].get_shape()[1].value

                output = add_layer(
                    layer_input,    # input
                    in_size,        # input size
                    N_HIDDEN_UNITS, # output size
                    ACTIVATION,     # activation function
                    norm,           # normalize before activation
                )
                layers_inputs.append(output)    # add output for next run

            # build output layer
            prediction = add_layer(layers_inputs[-1], 30, 1, activation_function=None)

            cost = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))
            train_op = tf.train.GradientDescentOptimizer(0.001).minimize(cost)
            return [train_op, cost, layers_inputs]

        # make up data
        fix_seed(1)
        x_data = np.linspace(-7, 10, 2500)[:, np.newaxis]
        np.random.shuffle(x_data)
        noise = np.random.normal(0, 8, x_data.shape)
        y_data = np.square(x_data) - 5 + noise

        # plot input data
        plt.scatter(x_data, y_data)
        plt.show()

        xs = tf.placeholder(tf.float32, [None, 1])  # [num_samples, num_features]
        ys = tf.placeholder(tf.float32, [None, 1])

        train_op, cost, layers_inputs = built_net(xs, ys, norm=False)   # without BN
        train_op_norm, cost_norm, layers_inputs_norm = built_net(xs, ys, norm=True) # with BN

        sess = tf.Session()
        if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:
            init = tf.initialize_all_variables()
        else:
            init = tf.global_variables_initializer()
        sess.run(init)

        # record cost
        cost_his = []
        cost_his_norm = []
        record_step = 5

        plt.ion()
        plt.figure(figsize=(7, 3))
        for i in range(250):
            if i % 50 == 0:
                # plot histogram
                all_inputs, all_inputs_norm = sess.run([layers_inputs, layers_inputs_norm], feed_dict={xs: x_data, ys: y_data})
                plot_his(all_inputs, all_inputs_norm)

            # train on batch
            sess.run([train_op, train_op_norm], feed_dict={xs: x_data[i*10:i*10+10], ys: y_data[i*10:i*10+10]})

            if i % record_step == 0:
                # record cost
                cost_his.append(sess.run(cost, feed_dict={xs: x_data, ys: y_data}))
                cost_his_norm.append(sess.run(cost_norm, feed_dict={xs: x_data, ys: y_data}))

        plt.ioff()
        plt.figure()
        plt.plot(np.arange(len(cost_his))*record_step, np.array(cost_his), label='no BN')     # no norm
        plt.plot(np.arange(len(cost_his))*record_step, np.array(cost_his_norm), label='BN')   # norm
        plt.legend()
        plt.show()

